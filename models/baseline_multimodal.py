"""
ğŸ¯ Step 1: Baselineå¤šæ¨¡æ€åºåˆ—æ¨èå™¨
æ¶æ„ï¼šText(BERT) + Image(ResNet) â†’ Concat â†’ GRU â†’ BPR

è®¾è®¡åŸåˆ™ï¼š
- å°½å¯èƒ½ç®€å•
- æ¨¡å—åŒ–ï¼ˆæ–¹ä¾¿åç»­æ›¿æ¢ï¼‰
- å®Œæ•´ï¼ˆåŒ…å«è®­ç»ƒ/è¯„ä¼°ï¼‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional


class BaselineMultimodalSeqRec(nn.Module):
    """
    Baselineå¤šæ¨¡æ€åºåˆ—æ¨èå™¨
    
    æµç¨‹ï¼š
    1. Text + Image â†’ Concat â†’ Projection
    2. Sequence â†’ GRU â†’ User representation
    3. User Ã— Item â†’ Score
    4. BPR Loss
    """
    
    def __init__(
        self,
        text_dim: int = 768,              # BERTç‰¹å¾ç»´åº¦
        image_dim: int = 2048,            # ResNetç‰¹å¾ç»´åº¦
        item_embedding_dim: int = 128,    # ç‰©å“åµŒå…¥ç»´åº¦
        hidden_dim: int = 256,            # GRUéšè—ç»´åº¦
        num_items: int = 10000,           # ç‰©å“æ€»æ•°
        dropout: float = 0.2,             # Dropoutç‡
        num_negatives: int = 100          # è´Ÿé‡‡æ ·æ•°é‡
    ):
        super().__init__()
        
        self.text_dim = text_dim
        self.image_dim = image_dim
        self.hidden_dim = hidden_dim
        self.num_items = num_items
        self.num_negatives = num_negatives
        
        # ===Step 1: å¤šæ¨¡æ€ç‰¹å¾èåˆï¼ˆæœ€ç®€å•ï¼šconcat + projectionï¼‰===
        multimodal_dim = text_dim + image_dim  # 768 + 2048 = 2816
        self.multimodal_proj = nn.Sequential(
            nn.Linear(multimodal_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # ===Step 2: åºåˆ—å»ºæ¨¡ï¼ˆGRUï¼‰===
        self.sequence_encoder = nn.GRU(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=1,                 # å•å±‚ï¼Œç®€å•
            batch_first=True,
            dropout=0.0,                   # å•å±‚ä¸éœ€è¦dropout
            bidirectional=True             # åŒå‘ï¼Œæ•ˆæœæ›´å¥½
        )
        
        # ===Step 3: Item Embeddingï¼ˆç”¨äºå€™é€‰æ‰“åˆ†ï¼‰===
        self.item_embedding = nn.Embedding(
            num_items + 1,  # +1 for padding
            item_embedding_dim,
            padding_idx=0
        )
        
        # ===Step 4: è¾“å‡ºæŠ•å½±ï¼ˆå¯¹é½ç»´åº¦ï¼‰===
        # åŒå‘GRUè¾“å‡ºæ˜¯ 2*hidden_dim
        self.user_proj = nn.Linear(hidden_dim * 2, item_embedding_dim)
        
        # Item biasï¼ˆæ¨èç³»ç»Ÿæ ‡é…ï¼Œèƒ½æå‡æ€§èƒ½ï¼‰
        self.item_bias = nn.Parameter(torch.zeros(num_items + 1))
        
        # åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        # Item embedding
        nn.init.xavier_normal_(self.item_embedding.weight[1:])  # è·³è¿‡padding
        nn.init.zeros_(self.item_embedding.weight[0])
        
        # Multimodal projection
        for module in self.multimodal_proj:
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.zeros_(module.bias)
        
        # GRU
        for name, param in self.sequence_encoder.named_parameters():
            if 'weight' in name:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.zeros_(param)
        
        # User projection
        nn.init.xavier_uniform_(self.user_proj.weight)
        nn.init.zeros_(self.user_proj.bias)
    
    def forward(
        self,
        text_seq: torch.Tensor,           # (batch, seq_len, 768)
        image_seq: torch.Tensor,          # (batch, seq_len, 2048)
        seq_lengths: torch.Tensor,        # (batch,)
        target_items: Optional[torch.Tensor] = None,  # (batch,) è®­ç»ƒç”¨
        candidate_items: Optional[torch.Tensor] = None,  # (batch, num_cand) è®­ç»ƒç”¨
        return_loss: bool = True
    ) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            text_seq: æ–‡æœ¬åºåˆ—ç‰¹å¾
            image_seq: å›¾åƒåºåˆ—ç‰¹å¾
            seq_lengths: æ¯ä¸ªåºåˆ—çš„å®é™…é•¿åº¦
            target_items: ç›®æ ‡item IDï¼ˆè®­ç»ƒæ—¶ç”¨ï¼‰
            candidate_items: å€™é€‰item IDï¼ˆè´Ÿé‡‡æ ·ï¼Œè®­ç»ƒæ—¶ç”¨ï¼‰
            return_loss: æ˜¯å¦è¿”å›æŸå¤±
        
        Returns:
            åŒ…å«userè¡¨ç¤ºå’Œé¢„æµ‹å¾—åˆ†çš„å­—å…¸
        """
        batch_size, seq_len, _ = text_seq.shape
        device = text_seq.device
        
        # ===Step 1: å¤šæ¨¡æ€èåˆ===
        # Concat text + image
        multimodal_feat = torch.cat([text_seq, image_seq], dim=-1)  # (batch, seq_len, 2816)
        
        # Projection
        multimodal_embed = self.multimodal_proj(multimodal_feat)  # (batch, seq_len, hidden_dim)
        
        # ===Step 2: åºåˆ—ç¼–ç ï¼ˆGRUï¼‰===
        # Pack padded sequence
        packed_input = nn.utils.rnn.pack_padded_sequence(
            multimodal_embed,
            seq_lengths.cpu(),
            batch_first=True,
            enforce_sorted=False
        )
        
        packed_output, hidden = self.sequence_encoder(packed_input)
        
        # Unpack
        output, _ = nn.utils.rnn.pad_packed_sequence(
            packed_output,
            batch_first=True,
            total_length=seq_len
        )
        
        # å–æœ€åæ—¶åˆ»çš„è¾“å‡ºä½œä¸ºç”¨æˆ·è¡¨ç¤º
        device = text_seq.device  # ä»è¾“å…¥tensorè·å–device
        batch_indices = torch.arange(batch_size, device=device)
        last_indices = (seq_lengths - 1).long().to(device)  # ç¡®ä¿åœ¨æ­£ç¡®çš„deviceä¸Š
        user_repr = output[batch_indices, last_indices]  # (batch, hidden_dim*2)
        
        # ===Step 3: æŠ•å½±åˆ°itemç©ºé—´===
        user_embed = self.user_proj(user_repr)  # (batch, item_embedding_dim)
        
        # L2å½’ä¸€åŒ–ï¼ˆæå‡ç¨³å®šæ€§ï¼‰
        user_embed = F.normalize(user_embed, p=2, dim=-1)
        
        # ===Step 4: æ‰“åˆ†===
        # è·å–æ‰€æœ‰itemçš„embeddingï¼ˆå½’ä¸€åŒ–ï¼‰
        all_item_embeds = self.item_embedding.weight  # (num_items+1, item_embedding_dim)
        all_item_embeds_norm = F.normalize(all_item_embeds, p=2, dim=-1)
        
        if candidate_items is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—å€™é€‰itemçš„å¾—åˆ†
            candidate_embeds = all_item_embeds_norm[candidate_items]  # (batch, num_cand, embed_dim)
            logits = torch.bmm(
                candidate_embeds,
                user_embed.unsqueeze(-1)
            ).squeeze(-1)  # (batch, num_cand)
            
            # åŠ bias
            logits = logits + self.item_bias[candidate_items]
        else:
            # æ¨ç†æ¨¡å¼ï¼šè®¡ç®—æ‰€æœ‰itemçš„å¾—åˆ†
            logits = torch.matmul(user_embed, all_item_embeds_norm.T)  # (batch, num_items+1)
            logits = logits + self.item_bias
            
            # Mask padding item
            logits[:, 0] = -1e9
        
        results = {
            'recommendation_logits': logits,
            'user_representation': user_embed
        }
        
        # ===Step 5: BPRæŸå¤±===
        if return_loss and target_items is not None and candidate_items is not None:
            # ç¬¬ä¸€ä¸ªæ˜¯æ­£æ ·æœ¬ï¼Œå…¶ä½™æ˜¯è´Ÿæ ·æœ¬
            pos_scores = logits[:, 0]  # (batch,)
            neg_scores = logits[:, 1:]  # (batch, num_neg)
            
            # BPRæŸå¤±ï¼š-log(Ïƒ(pos - neg))
            diff = pos_scores.unsqueeze(1) - neg_scores  # (batch, num_neg)
            loss = -F.logsigmoid(diff).mean()
            
            results['loss'] = loss
            results['bpr_loss'] = loss
        
        return results


# æµ‹è¯•å‡½æ•°
def test_baseline_model():
    """æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸è¿è¡Œ"""
    print("=" * 80)
    print("Testing BaselineMultimodalSeqRec")
    print("=" * 80)
    
    # è¶…å‚æ•°
    batch_size = 4
    seq_len = 10
    text_dim = 768
    image_dim = 2048
    num_items = 100
    num_negatives = 5
    
    # åˆ›å»ºæ¨¡å‹
    model = BaselineMultimodalSeqRec(
        text_dim=text_dim,
        image_dim=image_dim,
        item_embedding_dim=64,
        hidden_dim=128,
        num_items=num_items,
        dropout=0.2,
        num_negatives=num_negatives
    )
    
    print(f"âœ“ Model created")
    total_params = sum(p.numel() for p in model.parameters())
    print(f"âœ“ Total parameters: {total_params / 1e6:.2f}M")
    
    # æ¨¡æ‹Ÿæ•°æ®
    text_seq = torch.randn(batch_size, seq_len, text_dim)
    image_seq = torch.randn(batch_size, seq_len, image_dim)
    seq_lengths = torch.tensor([10, 8, 6, 9])
    target_items = torch.randint(1, num_items + 1, (batch_size,))
    
    # æ„å»ºå€™é€‰ç‰©å“ï¼ˆ1æ­£ + Kè´Ÿï¼‰
    candidate_items = torch.zeros(batch_size, num_negatives + 1, dtype=torch.long)
    candidate_items[:, 0] = target_items  # ç¬¬ä¸€ä¸ªæ˜¯æ­£æ ·æœ¬
    for i in range(batch_size):
        # éšæœºé‡‡æ ·è´Ÿæ ·æœ¬
        neg_items = torch.randint(1, num_items + 1, (num_negatives,))
        candidate_items[i, 1:] = neg_items
    
    print(f"âœ“ Test data created")
    
    # å‰å‘ä¼ æ’­ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰
    model.train()
    outputs = model(
        text_seq=text_seq,
        image_seq=image_seq,
        seq_lengths=seq_lengths,
        target_items=target_items,
        candidate_items=candidate_items,
        return_loss=True
    )
    
    print(f"âœ“ Forward pass (train mode) succeeded")
    print(f"  - Logits shape: {outputs['recommendation_logits'].shape}")
    print(f"  - User repr shape: {outputs['user_representation'].shape}")
    print(f"  - BPR Loss: {outputs['bpr_loss'].item():.4f}")
    
    # åå‘ä¼ æ’­
    loss = outputs['loss']
    loss.backward()
    print(f"âœ“ Backward pass succeeded")
    
    # å‰å‘ä¼ æ’­ï¼ˆè¯„ä¼°æ¨¡å¼ï¼‰
    model.eval()
    with torch.no_grad():
        outputs = model(
            text_seq=text_seq,
            image_seq=image_seq,
            seq_lengths=seq_lengths,
            target_items=None,
            candidate_items=None,
            return_loss=False
        )
    
    print(f"âœ“ Forward pass (eval mode) succeeded")
    print(f"  - Logits shape: {outputs['recommendation_logits'].shape}")
    print(f"  - Expected: ({batch_size}, {num_items + 1})")
    
    print("=" * 80)
    print("âœ… All tests passed!")
    print("=" * 80)


if __name__ == '__main__':
    test_baseline_model()

