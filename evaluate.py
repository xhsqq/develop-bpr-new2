"""
è¯„ä¼°è„šæœ¬ - åŠ è½½å·²è®­ç»ƒæ¨¡å‹å¹¶åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
Usage:
    python evaluate.py --checkpoint checkpoints/beauty_20231201_120000/best_model.pt --category beauty
"""

import torch
import argparse
import json
import os
from typing import Dict

from models.multimodal_recommender import MultimodalRecommender
from data.dataloader import get_dataloaders
from utils.evaluation import FullLibraryEvaluator, get_train_items_per_user


def load_model_from_checkpoint(checkpoint_path: str, device: str = 'cuda') -> tuple:
    """
    ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹

    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        device: è®¾å¤‡

    Returns:
        (model, checkpoint_dict)
    """
    print(f"Loading checkpoint from: {checkpoint_path}")
    # PyTorch 2.6 é»˜è®¤ weights_only=True ä¼šå¯¼è‡´åŒ…å«éæƒé‡å¯¹è±¡çš„å­—å…¸æ— æ³•ååºåˆ—åŒ–
    checkpoint = torch.load(checkpoint_path, weights_only=False, map_location=device)

    # ä»checkpointä¸­è·å–é…ç½®
    if 'args' in checkpoint:
        args = argparse.Namespace(**checkpoint['args'])
    else:
        raise ValueError("Checkpoint does not contain 'args'. Cannot reconstruct model.")

    # é‡å»ºæ¨¡å‹
    modality_dims = {
        'text': 768,
        'image': 2048
    }

    model = MultimodalRecommender(
        modality_dims=modality_dims,
        disentangled_dim=args.disentangled_dim,
        num_disentangled_dims=3,
        num_interests=args.num_interests,
        quantum_state_dim=args.quantum_state_dim,
        hidden_dim=args.hidden_dim,
        item_embed_dim=args.item_embed_dim,
        num_items=checkpoint['model_state_dict']['item_embedding.weight'].size(0) - 1,  # å‡å»padding
        max_seq_length=args.max_seq_length,
        alpha_recon=args.alpha_recon,
        alpha_causal=args.alpha_causal,
        alpha_diversity=args.alpha_diversity,
        use_quantum_computing=False
    ).to(device)

    # åŠ è½½æ¨¡å‹æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    print("âœ“ Model loaded successfully")
    print(f"  Epoch: {checkpoint.get('epoch', 'N/A')}")
    print(f"  Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")

    return model, checkpoint


def evaluate_model(
    model,
    test_loader,
    evaluator: FullLibraryEvaluator,
    device: str,
    train_items_per_user: Dict = None,
    show_progress: bool = True
) -> Dict[str, float]:
    """
    è¯„ä¼°æ¨¡å‹

    Args:
        model: æ¨èæ¨¡å‹
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        evaluator: è¯„ä¼°å™¨
        device: è®¾å¤‡
        train_items_per_user: è®­ç»ƒé›†ç‰©å“ï¼ˆç”¨äºè¿‡æ»¤ï¼‰
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦

    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    print("\n" + "=" * 80)
    print("Evaluating model on test set...")
    print("=" * 80)

    if train_items_per_user is not None:
        print("Using filtered evaluation (excluding training items)")
        metrics = evaluator.evaluate_with_filter(
            model, test_loader, train_items_per_user, device
        )
    else:
        print("Using full evaluation (including all items)")
        metrics = evaluator.evaluate(
            model, test_loader, device
        )

    return metrics


def main():
    parser = argparse.ArgumentParser(description='Evaluate trained multimodal recommender')

    # å¿…éœ€å‚æ•°
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='Path to checkpoint file (e.g., checkpoints/exp_name/best_model.pt)')
    parser.add_argument('--category', type=str, required=True,
                       choices=['beauty', 'games', 'sports'],
                       help='Amazon dataset category')

    # æ•°æ®å‚æ•°
    parser.add_argument('--data_dir', type=str, default='data/processed')
    parser.add_argument('--batch_size', type=int, default=256,
                       help='Batch size for evaluation')
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--use_text_features', action='store_true',
                       help='Use text features (slower)')

    # è¯„ä¼°å‚æ•°
    parser.add_argument('--filter_train_items', action='store_true',
                       help='Filter training items during evaluation')
    parser.add_argument('--k_list', type=int, nargs='+', default=[5, 10, 20, 50],
                       help='List of K values for Top-K evaluation')

    # å…¶ä»–
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')
    parser.add_argument('--output', type=str, default=None,
                       help='Output JSON file for results (default: same dir as checkpoint)')

    args = parser.parse_args()

    # æ£€æŸ¥æ£€æŸ¥ç‚¹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.checkpoint):
        raise FileNotFoundError(f"Checkpoint not found: {args.checkpoint}")

    print("\n" + "=" * 80)
    print("Multimodal Recommender - Model Evaluation")
    print("=" * 80)
    print(f"Checkpoint: {args.checkpoint}")
    print(f"Category: {args.category}")
    print(f"Device: {args.device}")
    print("=" * 80 + "\n")

    # åŠ è½½æ¨¡å‹
    model, checkpoint = load_model_from_checkpoint(args.checkpoint, args.device)

    # è·å–æ¨¡å‹é…ç½®ä¸­çš„ max_seq_length
    if 'args' in checkpoint:
        max_seq_length = checkpoint['args']['max_seq_length']
    else:
        max_seq_length = 50  # é»˜è®¤å€¼

    # åŠ è½½æ•°æ®
    print("\nLoading test data...")
    _, _, test_loader, dataset_info = get_dataloaders(
        category=args.category,
        data_dir=args.data_dir,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        max_seq_length=max_seq_length,
        use_text_features=args.use_text_features,
        num_negatives=0
    )

    print(f"âœ“ Test set: {dataset_info['test_size']} samples")
    print(f"âœ“ Dataset: {dataset_info['num_users']} users, {dataset_info['num_items']} items\n")

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = FullLibraryEvaluator(
        num_items=dataset_info['num_items'],
        k_list=args.k_list
    )

    # è·å–è®­ç»ƒé›†ç‰©å“ï¼ˆç”¨äºè¿‡æ»¤è¯„ä¼°ï¼‰
    train_items_per_user = None
    if args.filter_train_items:
        print("Building train item filters...")
        # éœ€è¦åŠ è½½è®­ç»ƒé›†
        train_loader, _, _, _ = get_dataloaders(
            category=args.category,
            data_dir=args.data_dir,
            batch_size=args.batch_size,
            num_workers=args.num_workers,
            max_seq_length=max_seq_length,
            use_text_features=args.use_text_features,
            num_negatives=0
        )
        train_items_per_user = get_train_items_per_user(train_loader.dataset)
        print(f"âœ“ Built filters for {len(train_items_per_user)} users\n")

    # è¯„ä¼°æ¨¡å‹
    test_metrics = evaluate_model(
        model,
        test_loader,
        evaluator,
        args.device,
        train_items_per_user
    )

    # æ‰“å°ç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š Test Results")
    print("=" * 80)
    for key, value in sorted(test_metrics.items()):
        print(f"  {key}: {value:.4f}")
    print("=" * 80 + "\n")

    # ä¿å­˜ç»“æœ
    if args.output is None:
        # é»˜è®¤ä¿å­˜åœ¨checkpointåŒç›®å½•
        checkpoint_dir = os.path.dirname(args.checkpoint)
        args.output = os.path.join(checkpoint_dir, 'evaluation_results.json')

    results = {
        'checkpoint': args.checkpoint,
        'category': args.category,
        'test_metrics': test_metrics,
        'config': {
            'filter_train_items': args.filter_train_items,
            'k_list': args.k_list,
            'batch_size': args.batch_size
        }
    }

    # å¦‚æœcheckpointä¸­æœ‰è®­ç»ƒæŒ‡æ ‡ï¼Œä¹Ÿä¿å­˜
    if 'valid_metrics' in checkpoint:
        results['valid_metrics'] = checkpoint['valid_metrics']
    if 'train_metrics' in checkpoint:
        results['train_metrics'] = checkpoint['train_metrics']

    with open(args.output, 'w') as f:
        json.dump(results, f, indent=4)

    print(f"âœ“ Results saved to: {args.output}\n")

    # å¦‚æœcheckpointä¸­æœ‰éªŒè¯é›†æŒ‡æ ‡ï¼Œæ¯”è¾ƒä¸€ä¸‹
    if 'valid_metrics' in checkpoint and 'NDCG@10' in checkpoint['valid_metrics']:
        valid_ndcg = checkpoint['valid_metrics']['NDCG@10']
        test_ndcg = test_metrics['NDCG@10']
        print("=" * 80)
        print("ğŸ“ˆ Performance Comparison")
        print("=" * 80)
        print(f"  Validation NDCG@10: {valid_ndcg:.4f}")
        print(f"  Test NDCG@10:       {test_ndcg:.4f}")
        print(f"  Difference:         {test_ndcg - valid_ndcg:+.4f}")
        print("=" * 80 + "\n")


if __name__ == '__main__':
    main()
